{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to Modelling - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "\n",
    "We can train our model using the different datasets and feature engineering techniques to evaluate their impact on the model performance.\n",
    "Print the score for the different methods we just introduced and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" print(\"Raw (but imputed) Features: {}\".\n",
    "      format(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid)))\n",
    "print(\"Normalized Features: {}\".\n",
    "      format(score_dataset(train_X_normalized, val_X_normalized, y_train, y_valid)))\n",
    "print(\"Standardized Features: {}\".\n",
    "      format(score_dataset(train_X_standardized, val_X_standardized, y_train, y_valid)))\n",
    "print(\"Log Landsize: {}\".format(score_dataset(train_X_logGains, val_X_logGains, y_train, y_valid))) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the MAE is, the better. So in this case, transforming the Landsize to a logarithmic scale (only one column!) has the biggest effect on the performance. It should be noted that in general it is recommended to try different combinations of the possible transformations, not only one each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Use standardized numeric features like in the previous section and combine it with OneHot Encoding and Label Encoding for training the model.\n",
    "\n",
    "The final results should have two cases: \n",
    "\n",
    "- One-hot encoded categorical + standardized numeric\n",
    "- Label encoded categorical + standardized numeric\n",
    "\n",
    "For each of the considered categorical columns, does it make more sense to use label encoding or one-hot encoding from a data perspective?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" num_X_train = X_train.drop(low_cardinality_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(low_cardinality_cols, axis=1)\n",
    "\n",
    "# standardize numerical columns\n",
    "scaler = StandardScaler()\n",
    "num_train_X_standardized = pd.DataFrame(scaler.fit_transform(num_X_train), \n",
    "                                        columns=num_X_train.columns, index=num_X_train.index)\n",
    "num_val_X_standardized = pd.DataFrame(scaler.transform(num_X_valid), \n",
    "                                      columns=num_X_valid.columns, index=num_X_valid.index)\n",
    "\n",
    "\n",
    "# One-hot encoding\n",
    "one_hot_X_train = pd.concat([num_train_X_standardized, one_hot_cols_train], axis=1)\n",
    "one_hot_X_valid = pd.concat([num_val_X_standardized, one_hot_cols_valid], axis=1)\n",
    "\n",
    "# Evaluate performance\n",
    "one_hot_encoding = score_dataset(one_hot_X_train.to_numpy(), one_hot_X_valid.to_numpy(), y_train, y_valid)\n",
    "print(\"MAE using One-hot Encoding and Standardization: {}\".format(one_hot_encoding))\n",
    "\n",
    "\n",
    "# Label encoding\n",
    "label_X_train_cat = pd.DataFrame(columns=low_cardinality_cols)\n",
    "label_X_valid_cat = pd.DataFrame(columns=low_cardinality_cols)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "for col in low_cardinality_cols:\n",
    "    label_X_train_cat[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid_cat[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "# align indices\n",
    "label_X_train_cat.index = num_train_X_standardized.index\n",
    "label_X_valid_cat.index = num_val_X_standardized.index\n",
    "\n",
    "label_X_train = pd.concat([num_train_X_standardized, label_X_train_cat], axis=1)\n",
    "label_X_valid = pd.concat([num_val_X_standardized, label_X_valid_cat], axis=1)\n",
    "\n",
    "# Evaluate performance\n",
    "label_encoding = score_dataset(label_X_train, label_X_valid, y_train, y_valid)\n",
    "print(\"MAE using Label Encoding and Standardization: {}\".format(mae_label_encoding)) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All three columns should be encoded with a One-hot Encoder since their categories don't have an explicit ordering. This is also reflected in the MAE of both the model with the standardized and the non-standardized numerical columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "1. If we have better information on the content of the categorical variables and time permits, we would not simply use the same encoder for all columns. Look at all columns of type \"object\" of the full data set (e.g. at the beginning of the notebook) and think about the adequate type of encoding (label or one-hot) for each of these columns. You find a description of the variables [here](https://www.kaggle.com/datasets/dansbecker/melbourne-housing-snapshot).\n",
    "2. (Homework, optional): Retrain the model using your own, more nuanced encoding approach and try whether you can obtain a better predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [cname for cname in X_train_full.columns if X_train_full[cname].dtype == \"object\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "__Suburb__ and __Adress__: no natural ordering, should be **one-hot encoded**\n",
    "\n",
    "__Type:__ br - bedroom(s); h - house,cottage,villa, semi,terrace; u - unit, duplex; t - townhouse; dev site - development site; o res - other residential. __The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "__Method:__ S - property sold; SP - property sold prior; PI - property passed in; PN - sold prior not disclosed; SN - sold not disclosed; NB - no bid; VB - vendor bid; W - withdrawn prior to auction; SA - sold after auction; SS - sold after auction price not disclosed. N/A - price or highest bid not available. __The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "__SellerG:__ Real Estate Agent __The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "__Date:__ Date sold __This is time-series data. It should be converted into a numerical variable (e.g by using [`pandas.to_datetime()`](https://pandas.pydata.org/docs/reference/api/pandas.to_datetime.html)__).\n",
    "\n",
    "__CouncilArea:__ Governing council for the area __The variable has no natural ordering --> use one-hot encoder__\n",
    "\n",
    "__Regionname:__ General Region (West, North West, North, North east â€¦etc)__The variable has no natural ordering --> use one-hot encoder__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Extend the pipeline by including the normalization step for the numerical features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this code to the original lecture notebook in order to execute it - it will not work here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" # Preprocessing numerical columns\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),  # Imputation\n",
    "    ('scaler', MinMaxScaler())                 # Normalization\n",
    "])\n",
    "\n",
    "# Preprocessing categorical columns\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Imputation\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))     # One-hot encoding\n",
    "])\n",
    "\n",
    "# Bundle both preprocessors\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_transformer, numerical_cols),  # Apply numerical pipeline\n",
    "    ('cat', categorical_transformer, low_cardinality_cols)  # Apply categorical pipeline\n",
    "])\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=1)\n",
    "\n",
    "\n",
    "# Bundle preprocessing and modeling code in a pipeline\n",
    "complete_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Preprocess the raw training data and fit the model\n",
    "complete_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Preprocess the raw validation data and make predictions\n",
    "preds = complete_pipeline.predict(X_valid)\n",
    "\n",
    "# Evaluate the model\n",
    "score = mean_absolute_error(y_valid, preds)\n",
    "print(\"MAE using the complete pipeline: {}\".format(score)) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
